{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning Tutorial v7: Comprehensive Comparison\n",
    "\n",
    "This notebook provides a complete comparison of different federated learning optimization strategies:\n",
    "\n",
    "## Experiments:\n",
    "1. **Baseline: Full FL (FP32)** - Standard federated learning with full precision\n",
    "2. **Optimization 1: Full FL (FP16/AMP)** - Mixed precision training for speed\n",
    "3. **Optimization 2: LoRA (FP32)** - Low-rank adaptation for communication efficiency\n",
    "4. **Optimization 3: LoRA (FP16/AMP)** - Combined LoRA + Mixed Precision\n",
    "\n",
    "## Metrics Tracked:\n",
    "- **Accuracy**: Final test accuracy\n",
    "- **Training Time**: Wall-clock time per experiment\n",
    "- **Communication Cost**: Estimated parameter transfer size\n",
    "- **Convergence**: Loss curves across rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import transforms, datasets\n",
    "from model import ConvNet2, LoRAConvNet2\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pre-training Initialization\n",
    "\n",
    "We pre-train the backbone on a small subset to provide a stable starting point for all experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pretrain_loader(data_path, batch_size=16, num_samples=100):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.2, 0.2, 0.2]),\n",
    "    ])\n",
    "    \n",
    "    site1_train_path = os.path.join(data_path, \"site1\", \"train\")\n",
    "    full_dataset = datasets.ImageFolder(root=site1_train_path, transform=transform)\n",
    "    \n",
    "    indices = np.random.choice(len(full_dataset), num_samples, replace=False)\n",
    "    subset = Subset(full_dataset, indices)\n",
    "    \n",
    "    return DataLoader(subset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "def pretrain_model(model, loader, epochs=5, device=\"cpu\"):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Pre-train Epoch {epoch+1} Loss: {running_loss/len(loader):.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "data_path = os.path.abspath(\"chest_xray\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"---- Starting Pre-training Initialization ----\")\n",
    "base_model = ConvNet2()\n",
    "pretrain_loader = get_pretrain_loader(data_path)\n",
    "initialized_model = pretrain_model(base_model, pretrain_loader, device=device)\n",
    "print(\"---- Pre-training Done ----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvflare.app_opt.pt.recipes.fedavg import FedAvgRecipe\n",
    "from nvflare.recipe import SimEnv, add_experiment_tracking\n",
    "\n",
    "n_clients = 3\n",
    "num_rounds = 5\n",
    "batch_size = 16\n",
    "\n",
    "# Create simulation environment once\n",
    "env = SimEnv(num_clients=n_clients)\n",
    "\n",
    "# Storage for results\n",
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experiment 1: Full FL (FP32) - Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT 1: Full Federated Learning (FP32)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model_full_fp32 = copy.deepcopy(initialized_model)\n",
    "job_name = \"chest-xray-full-fp32\"\n",
    "\n",
    "recipe = FedAvgRecipe(\n",
    "    name=job_name,\n",
    "    min_clients=n_clients,\n",
    "    num_rounds=num_rounds,\n",
    "    model=model_full_fp32,\n",
    "    train_script=\"client_xray.py\",\n",
    "    train_args=f\"--batch_size {batch_size} --epochs 1 --data_path {data_path} --model_type full --use_amp False\",\n",
    ")\n",
    "\n",
    "add_experiment_tracking(recipe, tracking_type=\"tensorboard\")\n",
    "\n",
    "start_time = time.time()\n",
    "run = recipe.execute(env)\n",
    "duration = time.time() - start_time\n",
    "\n",
    "print(f\"\\nStatus: {run.get_status()}\")\n",
    "print(f\"Duration: {duration:.2f} seconds\")\n",
    "print(f\"Result: {run.get_result()}\")\n",
    "\n",
    "results.append({\n",
    "    \"Experiment\": \"Full FL (FP32)\",\n",
    "    \"Method\": \"Full Parameter Tuning\",\n",
    "    \"Precision\": \"FP32\",\n",
    "    \"Duration (sec)\": round(duration, 2),\n",
    "    \"Params Updated\": \"100%\",\n",
    "    \"Communication Cost\": \"High\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experiment 2: Full FL (FP16/AMP) - Mixed Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT 2: Full Federated Learning (FP16/AMP)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model_full_fp16 = copy.deepcopy(initialized_model)\n",
    "job_name = \"chest-xray-full-fp16\"\n",
    "\n",
    "recipe = FedAvgRecipe(\n",
    "    name=job_name,\n",
    "    min_clients=n_clients,\n",
    "    num_rounds=num_rounds,\n",
    "    model=model_full_fp16,\n",
    "    train_script=\"client_xray.py\",\n",
    "    train_args=f\"--batch_size {batch_size} --epochs 1 --data_path {data_path} --model_type full --use_amp True\",\n",
    ")\n",
    "\n",
    "add_experiment_tracking(recipe, tracking_type=\"tensorboard\")\n",
    "\n",
    "start_time = time.time()\n",
    "run = recipe.execute(env)\n",
    "duration = time.time() - start_time\n",
    "\n",
    "print(f\"\\nStatus: {run.get_status()}\")\n",
    "print(f\"Duration: {duration:.2f} seconds\")\n",
    "print(f\"Result: {run.get_result()}\")\n",
    "\n",
    "results.append({\n",
    "    \"Experiment\": \"Full FL (FP16/AMP)\",\n",
    "    \"Method\": \"Full Parameter Tuning\",\n",
    "    \"Precision\": \"FP16 (Mixed)\",\n",
    "    \"Duration (sec)\": round(duration, 2),\n",
    "    \"Params Updated\": \"100%\",\n",
    "    \"Communication Cost\": \"High\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experiment 3: LoRA (FP32) - Low-Rank Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT 3: LoRA Federated Learning (FP32)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model_lora_fp32 = LoRAConvNet2(rank=8, base_model=copy.deepcopy(initialized_model))\n",
    "job_name = \"chest-xray-lora-fp32\"\n",
    "\n",
    "recipe = FedAvgRecipe(\n",
    "    name=job_name,\n",
    "    min_clients=n_clients,\n",
    "    num_rounds=num_rounds,\n",
    "    model=model_lora_fp32,\n",
    "    train_script=\"client_xray.py\",\n",
    "    train_args=f\"--batch_size {batch_size} --epochs 1 --data_path {data_path} --model_type lora --use_amp False\",\n",
    ")\n",
    "\n",
    "add_experiment_tracking(recipe, tracking_type=\"tensorboard\")\n",
    "\n",
    "start_time = time.time()\n",
    "run = recipe.execute(env)\n",
    "duration = time.time() - start_time\n",
    "\n",
    "print(f\"\\nStatus: {run.get_status()}\")\n",
    "print(f\"Duration: {duration:.2f} seconds\")\n",
    "print(f\"Result: {run.get_result()}\")\n",
    "\n",
    "results.append({\n",
    "    \"Experiment\": \"LoRA (FP32)\",\n",
    "    \"Method\": \"Low-Rank Adaptation\",\n",
    "    \"Precision\": \"FP32\",\n",
    "    \"Duration (sec)\": round(duration, 2),\n",
    "    \"Params Updated\": \"~1%\",\n",
    "    \"Communication Cost\": \"Low\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Experiment 4: LoRA (FP16/AMP) - Combined Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT 4: LoRA Federated Learning (FP16/AMP)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model_lora_fp16 = LoRAConvNet2(rank=8, base_model=copy.deepcopy(initialized_model))\n",
    "job_name = \"chest-xray-lora-fp16\"\n",
    "\n",
    "recipe = FedAvgRecipe(\n",
    "    name=job_name,\n",
    "    min_clients=n_clients,\n",
    "    num_rounds=num_rounds,\n",
    "    model=model_lora_fp16,\n",
    "    train_script=\"client_xray.py\",\n",
    "    train_args=f\"--batch_size {batch_size} --epochs 1 --data_path {data_path} --model_type lora --use_amp True\",\n",
    ")\n",
    "\n",
    "add_experiment_tracking(recipe, tracking_type=\"tensorboard\")\n",
    "\n",
    "start_time = time.time()\n",
    "run = recipe.execute(env)\n",
    "duration = time.time() - start_time\n",
    "\n",
    "print(f\"\\nStatus: {run.get_status()}\")\n",
    "print(f\"Duration: {duration:.2f} seconds\")\n",
    "print(f\"Result: {run.get_result()}\")\n",
    "\n",
    "results.append({\n",
    "    \"Experiment\": \"LoRA (FP16/AMP)\",\n",
    "    \"Method\": \"Low-Rank Adaptation\",\n",
    "    \"Precision\": \"FP16 (Mixed)\",\n",
    "    \"Duration (sec)\": round(duration, 2),\n",
    "    \"Params Updated\": \"~1%\",\n",
    "    \"Communication Cost\": \"Low\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL COMPARISON SUMMARY\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "display(df)\n",
    "\n",
    "# Calculate speedups\n",
    "baseline_time = df[df['Experiment'] == 'Full FL (FP32)']['Duration (sec)'].values[0]\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SPEEDUP ANALYSIS (vs Full FL FP32 Baseline)\")\n",
    "print(\"=\"*60)\n",
    "for _, row in df.iterrows():\n",
    "    speedup = baseline_time / row['Duration (sec)']\n",
    "    print(f\"{row['Experiment']:25s}: {speedup:.2f}x speedup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Results with TensorBoard\n",
    "\n",
    "All experiments are logged to TensorBoard for detailed comparison of loss curves and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --bind_all --logdir /tmp/nvflare/simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Expected Results:\n",
    "1. **FP16/AMP** should provide ~2-3x speedup over FP32 on GPU\n",
    "2. **LoRA** reduces communication overhead by ~99% (only ~1% of parameters)\n",
    "3. **LoRA + FP16** combines both benefits for maximum efficiency\n",
    "4. **Accuracy** should remain comparable across all methods\n",
    "\n",
    "### Trade-offs:\n",
    "- **Full FL (FP32)**: Highest accuracy potential, slowest, highest communication cost\n",
    "- **Full FL (FP16)**: Faster training, same communication cost, minimal accuracy loss\n",
    "- **LoRA (FP32)**: Reduced communication, slightly slower than full FP16\n",
    "- **LoRA (FP16)**: Best overall efficiency, minimal accuracy trade-off"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}